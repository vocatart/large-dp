# settings for obtaining validation data for each language
validation:
  validation_percentage: 0.01
  validation_minimum: 5
  validation_maximum: 100

# paths for saving data
# saved under experiments/exp_name
paths:
  checkpoint_dir: logs
  data_dir: data

# dp preprocessing
preprocessing:
  # number of grapheme repeats to allow for mapping to longer phoneme sequences
  # should always be 1 for autoregressive
  char_repeats: 1

  # sanitize to lowercase
  lowercase: true

  # default number of validation points if no validation data is provided
  n_val: 5000

# dp model
model:
  # ['transformer', 'autoreg_transformer']
  type: 'autoreg_transformer'

  d_model: 512
  d_fft: 1024
  layers: 4
  dropout: 0.1
  heads: 4

# dp training
training:
  # adam learning rate
  learning_rate: 0.0001

  # linear increase of lr from zero
  warmup_steps: 10000

  # factor to multiply learning rate on plateau
  scheduler_plateau_factor: 0.5

  # number of text generations with no improvement to tolerate
  scheduler_plateau_patience: 10

  # training & val batch size
  batch_size: 64
  batch_size_val: 32

  # epochs to train
  epochs: 500

  # interval to generate samples and calculate error rates
  generate_steps: 10000

  # interval to validate on
  validate_steps: 10000

  # interval to save model on
  checkpoint_steps: 100000

  # number of tensorboard samples shown
  n_generate_samples: 50

  store_phoneme_dict_in_model: true

  # torch ddp
  ddp_backend: 'nccl'
  ddp_host: 'localhost'
  ddp_post: '12355'